# Docker Best Practices - Version Agnostic Guide
## Mejores Pr√°cticas de Dockerizaci√≥n - Gu√≠a Universal

**Fecha de creaci√≥n:** 2025-10-02  
**Basado en:** Docker Official Documentation, OWASP, Google Cloud, AWS Best Practices  
**Aplicable a:** Cualquier stack tecnol√≥gico (Python, Node.js, Go, Java, etc.)  
**Filosof√≠a:** Principios atemporales, no atados a versiones espec√≠ficas

---

## üìö Fuentes Oficiales Consultadas

1. **Docker Official Documentation** - Best practices for writing Dockerfiles
2. **OWASP Container Security** - Security cheat sheet
3. **Google Cloud Run** - Container best practices
4. **AWS ECS/Fargate** - Container deployment guides
5. **CNCF (Cloud Native Computing Foundation)** - Cloud native patterns
6. **Snyk Container Security** - Vulnerability scanning guides

---

## üéØ Filosof√≠a de Dockerizaci√≥n

### Principios Fundamentales

1. **Inmutabilidad**: Los contenedores son ef√≠meros y reemplazables
2. **Reproducibilidad**: Mismo Dockerfile = misma imagen en cualquier entorno
3. **M√≠nimalismo**: Solo incluir lo necesario para ejecutar
4. **Seguridad por dise√±o**: Menor superficie de ataque posible
5. **Optimizaci√≥n**: Im√°genes peque√±as, r√°pidas de construir y desplegar

---

## üì¶ 1. SELECCI√ìN DE IM√ÅGENES BASE

### 1.1 Tipos de Im√°genes Base

#### Full Distribution
```dockerfile
# Ejemplo: debian, ubuntu, centos
FROM python:latest  # ‚ö†Ô∏è ~1GB, incluye compiladores y herramientas
```

**‚úÖ Usar cuando:**
- Necesitas compilar dependencias nativas
- Desarrollo y debugging
- Compatibilidad m√°xima

**‚ùå Evitar para:**
- Producci√≥n (demasiado grande)
- Im√°genes finales

#### Slim/Trimmed
```dockerfile
# Ejemplo: python-slim, node-slim
FROM python:slim  # ‚úÖ ~150-200MB, runtime b√°sico
```

**‚úÖ Usar cuando:**
- Producci√≥n con dependencias Python puras
- Balance entre tama√±o y compatibilidad
- La mayor√≠a de casos de uso

#### Alpine
```dockerfile
# Ejemplo: alpine, python-alpine, node-alpine
FROM python:alpine  # ‚ö° ~50MB, ultra-minimalista
```

**‚úÖ Usar cuando:**
- M√°xima optimizaci√≥n de tama√±o
- Sin dependencias nativas complejas
- Microservicios

**‚ö†Ô∏è Cuidado con:**
- Incompatibilidades de musl libc vs glibc
- Dependencias que requieren gcc/build-tools
- Tiempos de build m√°s largos (compilaci√≥n)

#### Distroless (Google)
```dockerfile
# Im√°genes sin shell, package manager
FROM gcr.io/distroless/python3
```

**‚úÖ Usar cuando:**
- Seguridad m√°xima (sin shell = sin shell exploits)
- Producci√≥n de alto riesgo
- Cumplimiento normativo estricto

**‚ùå Evitar para:**
- Debugging (no hay shell)
- Desarrollo

### 1.2 Estrategia de Versionado

#### ‚ùå MAL: Tag Latest
```dockerfile
FROM python:latest      # Imprevisible, cambia sin aviso
FROM node:latest        # Puede romper builds existentes
```

#### ‚úÖ BIEN: Versi√≥n Espec√≠fica
```dockerfile
# Patr√≥n: <imagen>:<major>.<minor>.<patch>-<variant>
FROM python:3.11.6-slim           # ‚úÖ Reproducible
FROM node:18.17.1-alpine          # ‚úÖ Predecible
FROM nginx:1.25.3-alpine          # ‚úÖ Espec√≠fico
```

#### ‚úÖ BUENO: Major.Minor (con vigilancia)
```dockerfile
FROM python:3.11-slim    # ‚ö†Ô∏è Actualiza patches autom√°ticamente
FROM node:18-alpine      # ‚ö†Ô∏è √ötil pero requiere testing
```

**Recomendaci√≥n**: Major.Minor para dependencias estables, Major.Minor.Patch para producci√≥n cr√≠tica.

### 1.3 Matriz de Decisi√≥n de Imagen Base

| Stack | Desarrollo | Staging | Producci√≥n |
|-------|-----------|---------|------------|
| **Python** | `python:X.Y-full` | `python:X.Y-slim` | `python:X.Y.Z-slim` o `alpine` |
| **Node.js** | `node:X.Y` | `node:X.Y-slim` | `node:X.Y.Z-alpine` |
| **Go** | `golang:X.Y` | `alpine` (compilado) | `scratch` o `distroless` |
| **Java** | `openjdk:X.Y` | `openjdk:X.Y-slim` | `openjdk:X.Y.Z-jre-slim` |
| **Nginx** | `nginx:X.Y` | `nginx:X.Y-alpine` | `nginx:X.Y.Z-alpine` |

---

## üèóÔ∏è 2. MULTI-STAGE BUILDS

### 2.1 Concepto Fundamental

**Problema**: Herramientas de build aumentan tama√±o innecesariamente en producci√≥n.

**Soluci√≥n**: Separar build y runtime en stages diferentes.

### 2.2 Patr√≥n B√°sico

```dockerfile
# ========================================
# Stage 1: Builder (build dependencies)
# ========================================
FROM <base-image>:<version> AS builder

# Instalar herramientas de compilaci√≥n
RUN install_build_tools

# Copiar archivos de dependencias
COPY dependency_files ./

# Instalar dependencias
RUN install_dependencies

# Copiar c√≥digo fuente
COPY source_code ./

# Compilar/Build
RUN build_application


# ========================================
# Stage 2: Runtime (solo lo necesario)
# ========================================
FROM <base-image>:<slim-version> AS runtime

# Crear usuario no-root
RUN create_app_user

# Copiar SOLO artefactos necesarios desde builder
COPY --from=builder /build/output /app/

# Cambiar a usuario no-root
USER app_user

# Comando de ejecuci√≥n
CMD ["run_application"]
```

### 2.3 Ejemplo: Python Backend (FastAPI/Django/Flask)

```dockerfile
# ========================================
# Stage 1: Dependencies Builder
# ========================================
FROM python:slim AS builder

WORKDIR /build

# Instalar dependencias de compilaci√≥n (si necesario)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc \
        libpq-dev && \
    rm -rf /var/lib/apt/lists/*

# Copiar solo archivos de dependencias (cache optimization)
COPY requirements.txt .

# Instalar dependencias en directorio de usuario
RUN pip install --no-cache-dir --user -r requirements.txt


# ========================================
# Stage 2: Production Runtime
# ========================================
FROM python:slim AS production

WORKDIR /app

# Instalar solo runtime dependencies (no gcc)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        libpq5 && \
    rm -rf /var/lib/apt/lists/*

# Crear usuario no-privilegiado
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

# Copiar dependencias instaladas desde builder
COPY --from=builder --chown=appuser:appuser /root/.local /home/appuser/.local

# Copiar c√≥digo de aplicaci√≥n
COPY --chown=appuser:appuser . .

# Configurar PATH para incluir paquetes de usuario
ENV PATH=/home/appuser/.local/bin:$PATH

# Cambiar a usuario no-root
USER appuser

# Exponer puerto (documentaci√≥n, no abre puerto)
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')" || exit 1

# Comando de ejecuci√≥n
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Beneficio**: ~850MB ‚Üí ~180MB (79% reducci√≥n)

### 2.4 Ejemplo: Node.js Frontend (Next.js/React/Vue)

```dockerfile
# ========================================
# Stage 1: Dependencies Installation
# ========================================
FROM node:alpine AS deps

WORKDIR /app

# Copiar solo archivos de dependencias (mejor cache)
COPY package.json package-lock.json* pnpm-lock.yaml* yarn.lock* ./

# Instalar dependencias seg√∫n package manager detectado
RUN \
  if [ -f pnpm-lock.yaml ]; then \
    corepack enable pnpm && pnpm install --frozen-lockfile; \
  elif [ -f yarn.lock ]; then \
    yarn install --frozen-lockfile; \
  else \
    npm ci; \
  fi


# ========================================
# Stage 2: Application Builder
# ========================================
FROM node:alpine AS builder

WORKDIR /app

# Copiar node_modules desde stage deps
COPY --from=deps /app/node_modules ./node_modules

# Copiar c√≥digo fuente
COPY . .

# Variables de entorno de build (si necesario)
ARG NEXT_PUBLIC_API_URL
ENV NEXT_PUBLIC_API_URL=$NEXT_PUBLIC_API_URL

# Build de aplicaci√≥n
RUN \
  if [ -f pnpm-lock.yaml ]; then \
    corepack enable pnpm && pnpm build; \
  elif [ -f yarn.lock ]; then \
    yarn build; \
  else \
    npm run build; \
  fi


# ========================================
# Stage 3: Production Runtime
# ========================================
FROM node:alpine AS runner

WORKDIR /app

ENV NODE_ENV=production

# Crear usuario no-root
RUN addgroup --system --gid 1001 nodejs && \
    adduser --system --uid 1001 nextjs

# Copiar SOLO archivos de producci√≥n
# Para Next.js con output: 'standalone'
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static
COPY --from=builder --chown=nextjs:nodejs /app/public ./public

# Cambiar a usuario no-root
USER nextjs

EXPOSE 3000

ENV PORT 3000
ENV HOSTNAME "0.0.0.0"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD node -e "require('http').get('http://localhost:3000/api/health', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })"

# Comando de ejecuci√≥n
CMD ["node", "server.js"]
```

**Beneficio**: ~1.2GB ‚Üí ~120MB (90% reducci√≥n)

### 2.5 Ejemplo: Aplicaci√≥n Compilada (Go/Rust)

```dockerfile
# ========================================
# Stage 1: Builder
# ========================================
FROM golang:latest AS builder

WORKDIR /build

# Copiar archivos de dependencias
COPY go.mod go.sum ./
RUN go mod download

# Copiar c√≥digo fuente
COPY . .

# Compilar aplicaci√≥n est√°tica
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .


# ========================================
# Stage 2: Runtime (imagen m√≠nima)
# ========================================
FROM scratch

# Copiar certificados SSL (si necesario para HTTPS)
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/

# Copiar binario compilado
COPY --from=builder /build/app /app

# Exponer puerto
EXPOSE 8080

# Comando de ejecuci√≥n
ENTRYPOINT ["/app"]
```

**Beneficio**: ~800MB ‚Üí ~10MB (99% reducci√≥n)

---

## üîí 3. SEGURIDAD

### 3.1 Usuarios No-Root (CR√çTICO)

#### ‚ùå MAL: Ejecutar como Root
```dockerfile
FROM python:slim
COPY . /app
WORKDIR /app
CMD ["python", "app.py"]  # Ejecuta como root (UID 0)
```

**Riesgo**: Si atacante compromete contenedor, tiene permisos root.

#### ‚úÖ BIEN: Crear y Usar Usuario No-Privilegiado
```dockerfile
FROM python:slim

# Crear grupo y usuario con UID/GID espec√≠ficos
RUN groupadd -r appgroup -g 1000 && \
    useradd -r -u 1000 -g appgroup -m -s /bin/bash appuser

# Crear directorios con permisos apropiados
RUN mkdir -p /app && chown -R appuser:appgroup /app

WORKDIR /app

# Copiar archivos con ownership correcto
COPY --chown=appuser:appgroup . .

# Cambiar a usuario no-root ANTES de CMD
USER appuser

CMD ["python", "app.py"]  # Ejecuta como appuser (UID 1000)
```

#### Verificar Usuario
```bash
# Verificar que contenedor NO corre como root
docker run myimage id
# Debe mostrar: uid=1000(appuser) gid=1000(appgroup)
# NO debe mostrar: uid=0(root)
```

### 3.2 Gesti√≥n de Secretos

#### ‚ùå NUNCA HACER ESTO:
```dockerfile
# ‚ùå Secretos hardcoded en Dockerfile
ENV DB_PASSWORD=supersecret123
ENV API_KEY=abc123xyz

# ‚ùå Secretos en argumentos de build
ARG PRIVATE_KEY="-----BEGIN PRIVATE KEY-----..."

# ‚ùå Copiar archivos con secretos
COPY .env /app/.env
```

**Problema**: Secretos quedan en layers de imagen (permanentes, extra√≠bles).

#### ‚úÖ SOLUCIONES SEGURAS:

##### Opci√≥n 1: Variables de Entorno en Runtime
```bash
# docker run
docker run -e DB_PASSWORD=$(cat secret.txt) myimage

# docker-compose
docker-compose run -e DB_PASSWORD=secret myimage
```

##### Opci√≥n 2: Docker Secrets (Swarm/Compose)
```yaml
# docker-compose.yml
services:
  app:
    image: myimage
    secrets:
      - db_password
      - api_key

secrets:
  db_password:
    external: true  # Gestionado externamente
  api_key:
    file: ./secrets/api_key.txt  # Archivo local (NO committear)
```

```dockerfile
# Dockerfile - leer secrets en runtime
FROM python:slim
# Secrets se montan en /run/secrets/ autom√°ticamente
CMD ["sh", "-c", "export DB_PASSWORD=$(cat /run/secrets/db_password) && python app.py"]
```

##### Opci√≥n 3: Proveedores Externos
- **AWS Secrets Manager**
- **Azure Key Vault**
- **Google Secret Manager**
- **HashiCorp Vault**

```python
# app.py - obtener secrets desde proveedor
import boto3

def get_secret(secret_name):
    client = boto3.client('secretsmanager')
    response = client.get_secret_value(SecretId=secret_name)
    return response['SecretString']

db_password = get_secret('prod/db/password')
```

### 3.3 Escaneo de Vulnerabilidades

#### Herramientas Recomendadas

##### Trivy (Open Source)
```bash
# Instalar
curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh

# Escanear imagen
trivy image myimage:latest

# Escanear solo severidad CRITICAL y HIGH
trivy image --severity CRITICAL,HIGH myimage:latest

# Fallar build si hay CRITICAL
trivy image --exit-code 1 --severity CRITICAL myimage:latest
```

##### Docker Scan (Snyk)
```bash
# Escanear imagen
docker scan myimage:latest

# Escanear con severidad
docker scan --severity=high myimage:latest
```

##### Clair (Red Hat)
```bash
# Setup con docker-compose
docker-compose -f clair-compose.yml up -d

# Escanear
clairctl analyze myimage:latest
```

#### Integraci√≥n CI/CD
```yaml
# .github/workflows/docker-security.yml
name: Docker Security Scan

on: [push, pull_request]

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Build image
        run: docker build -t myimage:${{ github.sha }} .
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'myimage:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '1'  # Fail if vulnerabilities found
      
      - name: Upload results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
```

### 3.4 Minimizar Superficie de Ataque

```dockerfile
# ‚ùå MAL: Instalar paquetes innecesarios
FROM ubuntu:latest
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    wget \
    vim \
    nano \
    git \
    ssh \
    sudo

# ‚úÖ BIEN: Solo lo necesario para runtime
FROM python:slim
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 && \  # Solo runtime library (no dev)
    rm -rf /var/lib/apt/lists/*  # Limpiar cache
```

**Principio**: Menos paquetes = menos CVEs potenciales

---

## ‚ö° 4. OPTIMIZACI√ìN DE TAMA√ëO Y CACHE

### 4.1 Order Matters: Optimizar Layer Caching

Docker cachea layers. Si un layer cambia, todos los siguientes se invalidan.

#### ‚ùå MAL: Orden Sub√≥ptimo
```dockerfile
FROM python:slim

# Layer 1: Copia TODO (cambia frecuentemente)
COPY . /app  # ‚ö†Ô∏è Invalida cache en cada cambio de c√≥digo

# Layer 2: Instala dependencias (cambia raramente)
WORKDIR /app
RUN pip install -r requirements.txt  # üîÑ Se reinstala innecesariamente
```

**Problema**: Cada cambio de c√≥digo reinstala dependencias (lento).

#### ‚úÖ BIEN: Orden √ìptimo
```dockerfile
FROM python:slim

WORKDIR /app

# Layer 1: Copia SOLO archivos de dependencias (cambia raramente)
COPY requirements.txt .  # ‚úÖ Cache se mantiene si requirements no cambia

# Layer 2: Instala dependencias
RUN pip install --no-cache-dir -r requirements.txt  # ‚úÖ Cacheado

# Layer 3: Copia c√≥digo (cambia frecuentemente)
COPY . .  # ‚ö†Ô∏è Solo invalida este layer
```

**Beneficio**: Dependencias solo se reinstalan cuando `requirements.txt` cambia.

### 4.2 Combinar RUN Commands

Cada `RUN` crea un layer nuevo. M√°s layers = imagen m√°s grande.

#### ‚ùå MAL: M√∫ltiples RUN
```dockerfile
RUN apt-get update                    # Layer 1: ~50MB
RUN apt-get install -y curl           # Layer 2: +5MB
RUN apt-get install -y wget           # Layer 3: +3MB
RUN apt-get clean                     # Layer 4: NO reduce tama√±o (layers inmutables)
RUN rm -rf /var/lib/apt/lists/*       # Layer 5: NO reduce tama√±o

# Total: 58MB + metadata
```

**Problema**: Limpieza no reduce tama√±o porque layers anteriores persisten.

#### ‚úÖ BIEN: RUN Combinado
```dockerfile
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        wget && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Total: 8MB (limpieza en mismo layer)
```

**Principio**: Crear + limpiar en mismo layer = tama√±o m√≠nimo

### 4.3 .dockerignore (OBLIGATORIO)

Equivalente a `.gitignore`, excluye archivos del build context.

```
# .dockerignore

# Dependencies
node_modules/
__pycache__/
*.pyc
*.pyo
.venv/
venv/
env/

# Git
.git/
.gitignore
.gitattributes

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Testing
.pytest_cache/
.coverage
coverage/
htmlcov/
.tox/
*.test
test-results/

# CI/CD
.github/
.gitlab-ci.yml
.travis.yml

# Documentation
*.md
!README.md
docs/

# Logs
*.log
logs/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Environment files
.env
.env.*
!.env.example

# Build artifacts
dist/
build/
*.egg-info/
.next/
out/

# OS
.DS_Store
Thumbs.db
```

**Beneficio**: Reduce build context de ~500MB a ~50MB t√≠picamente.

### 4.4 T√©cnicas Adicionales de Optimizaci√≥n

#### No Cachear Paquetes
```dockerfile
# Python
RUN pip install --no-cache-dir -r requirements.txt

# Node.js
RUN npm ci --omit=dev

# apt-get
RUN apt-get update && apt-get install -y --no-install-recommends \
    package && \
    rm -rf /var/lib/apt/lists/*
```

#### Usar COPY en vez de ADD
```dockerfile
# ‚ùå ADD tiene funcionalidad extra (descomprime .tar, descarga URLs)
ADD file.tar.gz /app/  # Puede comportarse inesperadamente

# ‚úÖ COPY es expl√≠cito y predecible
COPY file.tar.gz /app/
RUN tar -xzf /app/file.tar.gz  # Control expl√≠cito
```

---

## üè• 5. HEALTH CHECKS

### 5.1 Dockerfile HEALTHCHECK

```dockerfile
HEALTHCHECK [OPTIONS] CMD <command>

# Opciones:
#   --interval=DURATION (default: 30s)
#   --timeout=DURATION (default: 30s)
#   --start-period=DURATION (default: 0s)
#   --retries=N (default: 3)
```

### 5.2 Ejemplos por Stack

#### HTTP API (Python/Node.js/Go)
```dockerfile
# Opci√≥n 1: Con curl (si disponible)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Opci√≥n 2: Sin curl (Python)
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')" || exit 1

# Opci√≥n 3: Sin curl (Node.js)
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD node -e "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1))"
```

#### Base de Datos
```dockerfile
# PostgreSQL
HEALTHCHECK --interval=10s --timeout=5s --retries=5 \
    CMD pg_isready -U postgres || exit 1

# MongoDB
HEALTHCHECK --interval=10s --timeout=5s --retries=5 \
    CMD mongo --eval "db.adminCommand('ping')" || exit 1

# Redis
HEALTHCHECK --interval=10s --timeout=5s --retries=5 \
    CMD redis-cli ping || exit 1
```

### 5.3 Endpoint de Health en Aplicaci√≥n

```python
# Python (FastAPI)
@app.get("/health")
async def health_check():
    """
    Health check endpoint
    Returns 200 if service is healthy
    """
    checks = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # Check database
    try:
        await db.execute("SELECT 1")
        checks["checks"]["database"] = "connected"
    except Exception as e:
        checks["checks"]["database"] = f"error: {str(e)}"
        checks["status"] = "unhealthy"
    
    # Check external API
    try:
        response = await httpx.get("https://api.external.com/ping", timeout=5)
        checks["checks"]["external_api"] = "available" if response.status_code == 200 else "unavailable"
    except Exception as e:
        checks["checks"]["external_api"] = f"error: {str(e)}"
        checks["status"] = "unhealthy"
    
    status_code = 200 if checks["status"] == "healthy" else 503
    return JSONResponse(content=checks, status_code=status_code)
```

```javascript
// Node.js (Express)
app.get('/health', async (req, res) => {
    const checks = {
        status: 'healthy',
        timestamp: new Date().toISOString(),
        checks: {}
    };
    
    // Check database
    try {
        await db.query('SELECT 1');
        checks.checks.database = 'connected';
    } catch (error) {
        checks.checks.database = `error: ${error.message}`;
        checks.status = 'unhealthy';
    }
    
    const statusCode = checks.status === 'healthy' ? 200 : 503;
    res.status(statusCode).json(checks);
});
```

---

## üê≥ 6. DOCKER COMPOSE

### 6.1 Estructura de Archivos

```
project/
‚îú‚îÄ‚îÄ docker-compose.yml              # Base configuration
‚îú‚îÄ‚îÄ docker-compose.override.yml     # Local development (auto-merged)
‚îú‚îÄ‚îÄ docker-compose.dev.yml          # Development explicit
‚îú‚îÄ‚îÄ docker-compose.staging.yml      # Staging
‚îú‚îÄ‚îÄ docker-compose.prod.yml         # Production
‚îî‚îÄ‚îÄ .env.example                    # Template de variables
```

### 6.2 docker-compose.yml Base

```yaml
version: '3.8'

# ========================================
# Services
# ========================================
services:
  
  # Frontend Service
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production  # Multi-stage target
    image: ${PROJECT_NAME:-myapp}-frontend:${VERSION:-latest}
    container_name: ${PROJECT_NAME:-myapp}-frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - NEXT_PUBLIC_API_URL=${API_URL}
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - frontend-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  
  # Backend Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    image: ${PROJECT_NAME:-myapp}-backend:${VERSION:-latest}
    container_name: ${PROJECT_NAME:-myapp}-backend
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    environment:
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - frontend-network
      - backend-network
    volumes:
      - app-data:/app/data
      - app-logs:/app/logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  
  # Database Service
  database:
    image: postgres:alpine
    container_name: ${PROJECT_NAME:-myapp}-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${DB_NAME}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend-network
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
    secrets:
      - db_password
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  
  # Redis Cache
  redis:
    image: redis:alpine
    container_name: ${PROJECT_NAME:-myapp}-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - backend-network
    volumes:
      - redis-data:/data
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

# ========================================
# Networks
# ========================================
networks:
  frontend-network:
    driver: bridge
  backend-network:
    driver: bridge
    internal: true  # No internet access

# ========================================
# Volumes
# ========================================
volumes:
  app-data:
    driver: local
  app-logs:
    driver: local
  db-data:
    driver: local
  redis-data:
    driver: local

# ========================================
# Secrets
# ========================================
secrets:
  db_password:
    file: ./secrets/db_password.txt
```

### 6.3 docker-compose.dev.yml (Development Override)

```yaml
version: '3.8'

services:
  frontend:
    build:
      target: development  # Override target
    volumes:
      - ./frontend:/app  # Hot reload
      - /app/node_modules  # Exclude node_modules
    environment:
      - NODE_ENV=development
    command: npm run dev
  
  backend:
    build:
      target: development
    volumes:
      - ./backend:/app  # Hot reload
      - /app/__pycache__  # Exclude cache
    environment:
      - ENVIRONMENT=development
      - DEBUG=true
    command: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
      - "5678:5678"  # Debug port
  
  database:
    ports:
      - "5432:5432"  # Expose para debugging
```

### 6.4 docker-compose.prod.yml (Production Override)

```yaml
version: '3.8'

services:
  frontend:
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
  
  backend:
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
  
  database:
    volumes:
      - /mnt/db-backup:/backup:ro  # Read-only backup mount
```

### 6.5 Comandos de Compose

```bash
# ========================================
# Development
# ========================================
# Start con override autom√°tico (dev)
docker-compose up -d

# Start con archivo espec√≠fico
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d

# Ver logs
docker-compose logs -f backend

# Rebuild espec√≠fico
docker-compose up -d --build backend


# ========================================
# Production
# ========================================
# Start producci√≥n
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Scale service
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale backend=5


# ========================================
# Maintenance
# ========================================
# Stop todos los servicios
docker-compose down

# Stop y eliminar vol√∫menes (‚ö†Ô∏è CUIDADO)
docker-compose down -v

# Ver estado
docker-compose ps

# Ejecutar comando en servicio
docker-compose exec backend bash
docker-compose exec database psql -U postgres

# Ver recursos
docker-compose top
```

---

## üìä 7. RESOURCE LIMITS

### 7.1 Por qu√© Limitar Recursos

Sin l√≠mites, un contenedor puede:
- Consumir toda la RAM ‚Üí OOM kill otros contenedores
- Usar 100% CPU ‚Üí degradar sistema completo
- Llenar disco ‚Üí crash del host

### 7.2 Docker Run

```bash
# CPU limits
docker run --cpus="1.5" myimage            # Max 1.5 CPUs
docker run --cpu-shares=512 myimage        # Priority relativa

# Memory limits
docker run -m 512m myimage                 # Max 512MB RAM
docker run -m 512m --memory-swap 1g myimage  # Max 512MB RAM + 512MB swap

# Combinado
docker run \
  --cpus="2.0" \
  -m 1g \
  --memory-swap 2g \
  --restart unless-stopped \
  myimage
```

### 7.3 Docker Compose

```yaml
services:
  app:
    image: myimage
    deploy:
      resources:
        limits:
          cpus: '2.0'        # M√°ximo 2 CPUs
          memory: 1G          # M√°ximo 1GB RAM
        reservations:
          cpus: '0.5'         # M√≠nimo garantizado
          memory: 256M
```

### 7.4 Matriz de Recomendaciones

| Service Type | CPU Limits | Memory Limits | Notes |
|--------------|-----------|---------------|-------|
| **Frontend (Node.js)** | 0.5-1.0 | 256M-512M | Ajustar seg√∫n concurrencia |
| **Backend API** | 1.0-2.0 | 512M-2G | Depende de carga |
| **Worker/Queue** | 1.0-2.0 | 512M-1G | Por worker |
| **Database** | 2.0-4.0 | 2G-8G | No limitar si es √∫nico |
| **Redis/Cache** | 0.5-1.0 | 256M-1G | Seg√∫n cache size |
| **Nginx/Proxy** | 0.25-0.5 | 128M-256M | Muy eficiente |

---

## üîç 8. LOGGING & MONITORING

### 8.1 Logging Drivers

```yaml
services:
  app:
    logging:
      driver: "json-file"  # Default, parse-able
      options:
        max-size: "10m"    # Rotar cada 10MB
        max-file: "3"      # Mantener 3 archivos
        labels: "production"
        env: "APP_VERSION"
```

**Opciones de drivers:**
- `json-file`: Default, f√°cil parsing
- `syslog`: Enviar a syslog remoto
- `journald`: Systemd journal
- `gelf`: Graylog Extended Log Format
- `fluentd`: Fluentd collector
- `awslogs`: CloudWatch
- `gcplogs`: Google Cloud Logging

### 8.2 Formato de Logs Estructurados

```python
# Python - Logging estructurado JSON
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "backend",
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_data)

# Configurar
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger()
logger.addHandler(handler)
logger.setLevel(logging.INFO)
```

```javascript
// Node.js - Winston con formato JSON
const winston = require('winston');

const logger = winston.createLogger({
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'frontend' },
  transports: [
    new winston.transports.Console()
  ]
});

logger.info('User logged in', { userId: 123, method: 'OAuth' });
// Output: {"level":"info","message":"User logged in","service":"frontend","timestamp":"2025-10-02T12:00:00.000Z","userId":123,"method":"OAuth"}
```

### 8.3 Ver Logs

```bash
# Ver logs en tiempo real
docker logs -f container_name

# Ver √∫ltimas N l√≠neas
docker logs --tail 100 container_name

# Ver logs con timestamps
docker logs -t container_name

# Ver logs entre fechas
docker logs --since 2025-10-01 --until 2025-10-02 container_name

# Compose
docker-compose logs -f backend
docker-compose logs --tail=50 backend frontend
```

---

## üöÄ 9. CI/CD INTEGRATION

### 9.1 Pipeline Est√°ndar

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Lint         ‚îÇ  hadolint, dockerfile_lint
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2. Build        ‚îÇ  docker build multi-stage
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 3. Scan         ‚îÇ  Trivy, Snyk, Clair
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 4. Test         ‚îÇ  Run tests in container
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 5. Push         ‚îÇ  Registry (Docker Hub, ECR, GCR)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 6. Deploy       ‚îÇ  Staging ‚Üí Production
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 9.2 GitHub Actions Example

```yaml
name: Docker Build & Deploy

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  
  # ========================================
  # Job 1: Lint Dockerfile
  # ========================================
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Lint Dockerfile (hadolint)
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile
          failure-threshold: warning
  
  # ========================================
  # Job 2: Build & Scan
  # ========================================
  build:
    needs: lint
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      security-events: write
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
      
      - name: Build image
        uses: docker/build-push-action@v4
        with:
          context: .
          load: true  # Load para scanning local
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
          cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.meta.outputs.tags }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '1'  # Fail build si vulnerabilidades
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run tests in container
        run: |
          docker run --rm ${{ steps.meta.outputs.tags }} pytest
      
      - name: Push image
        if: github.event_name != 'pull_request'
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
  
  # ========================================
  # Job 3: Deploy (si push a main)
  # ========================================
  deploy:
    needs: build
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to production
        run: |
          echo "Deploying to production..."
          # SSH to server, docker-compose pull, restart, etc.
```

### 9.3 Image Tagging Strategy

```bash
# Estrategias comunes:

# 1. SemVer (Semantic Versioning)
myapp:1.2.3
myapp:1.2
myapp:1

# 2. Git Commit SHA
myapp:sha-a1b2c3d

# 3. Branch name
myapp:main
myapp:develop

# 4. Build number
myapp:build-123

# 5. Timestamp
myapp:20251002-120000

# 6. Combinado (RECOMENDADO)
myapp:1.2.3-20251002-a1b2c3d
#     ^      ^         ^
#     version timestamp  commit
```

---

## üìã 10. CHECKLIST DE PRODUCCI√ìN

### 10.1 Dockerfile

- [ ] **Multi-stage build** implementado
- [ ] **Imagen base espec√≠fica** (no `latest`)
- [ ] **Imagen slim/alpine** para producci√≥n
- [ ] **Usuario no-root** creado y usado
- [ ] **`.dockerignore`** presente y completo
- [ ] **HEALTHCHECK** configurado
- [ ] **Layers minimizados** (RUN combinados)
- [ ] **Cache optimizado** (COPY orden correcto)
- [ ] **Secretos externalizados** (no en imagen)
- [ ] **Dependencies limpias** (--no-cache-dir, rm apt lists)
- [ ] **Labels** informativos (versi√≥n, maintainer, etc.)
- [ ] **EXPOSE** documentado
- [ ] **CMD/ENTRYPOINT** correcto

### 10.2 Security

- [ ] **Imagen escaneada** (0 CRITICAL vulnerabilities)
- [ ] **Usuario no-root verificado** (`docker run myimage id`)
- [ ] **Secrets en runtime** (no hardcoded)
- [ ] **Puertos m√≠nimos** expuestos
- [ ] **Read-only filesystem** (si posible)
- [ ] **Capabilities dropped** (si posible)
- [ ] **No shells innecesarios** (distroless si posible)

### 10.3 Performance

- [ ] **Tama√±o < 500MB** (idealmente < 200MB)
- [ ] **Build time < 5min**
- [ ] **Startup time < 30s**
- [ ] **Resource limits** configurados
- [ ] **Health checks** respondiendo < 2s

### 10.4 Observability

- [ ] **Health endpoint** implementado
- [ ] **Logging estructurado** (JSON)
- [ ] **Log rotation** configurado
- [ ] **Metrics exposed** (Prometheus/StatsD)
- [ ] **Tracing** instrumentado (opcional)

### 10.5 Docker Compose

- [ ] **Versiones espec√≠ficas** de im√°genes
- [ ] **Healthchecks** configurados
- [ ] **Resource limits** establecidos
- [ ] **Restart policies** definidas
- [ ] **Secrets** externalizados
- [ ] **Vol√∫menes** para persistencia
- [ ] **Redes aisladas** por funci√≥n
- [ ] **Logging driver** configurado
- [ ] **Depends_on** con health conditions

---

## üéì 11. ANTI-PATTERNS (Evitar)

### ‚ùå 1. Running as Root
```dockerfile
# MAL
FROM python
COPY . /app
CMD ["python", "app.py"]  # Ejecuta como root
```

### ‚ùå 2. Using :latest Tag
```dockerfile
# MAL
FROM python:latest  # Imprevisible
```

### ‚ùå 3. Installing Unnecessary Packages
```dockerfile
# MAL
RUN apt-get update && apt-get install -y \
    python3 curl wget vim git ssh sudo build-essential
```

### ‚ùå 4. Hardcoded Secrets
```dockerfile
# MAL
ENV API_KEY=abc123xyz
ENV DB_PASSWORD=supersecret
```

### ‚ùå 5. Single Large Stage
```dockerfile
# MAL - Todo en un stage
FROM python
RUN apt-get install build-essential gcc  # 500MB
COPY . /app
RUN pip install -r requirements.txt
CMD ["python", "app.py"]
# Imagen final: 1.2GB con herramientas de build innecesarias
```

### ‚ùå 6. Poor Layer Caching
```dockerfile
# MAL - Orden sub√≥ptimo
COPY . /app  # Cambia frecuentemente, invalida todo
RUN pip install -r requirements.txt  # Se reinstala siempre
```

### ‚ùå 7. No .dockerignore
```
# Sin .dockerignore
# Build context incluye: node_modules/ (500MB), .git/ (200MB), logs/ (100MB)
# Build context total: 800MB
# Tiempo de env√≠o al daemon: 30s+
```

### ‚ùå 8. Exposing Unnecessary Ports
```dockerfile
# MAL
EXPOSE 22 3000 3306 5432 6379 8000 8080
```

### ‚ùå 9. No Health Checks
```yaml
# MAL - Sin healthcheck
services:
  app:
    image: myimage
    # No healthcheck = no auto-recovery
```

### ‚ùå 10. No Resource Limits
```yaml
# MAL
services:
  app:
    image: myimage
    # Sin limits = puede consumir todo el sistema
```

---

## üìö 12. RECURSOS Y REFERENCIAS

### Documentaci√≥n Oficial

1. **Docker**
   - Best Practices: https://docs.docker.com/develop/dev-best-practices/
   - Dockerfile Reference: https://docs.docker.com/engine/reference/builder/
   - Multi-stage Builds: https://docs.docker.com/build/building/multi-stage/

2. **Security**
   - OWASP Container Security: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html
   - CIS Docker Benchmark: https://www.cisecurity.org/benchmark/docker
   - Snyk Container Security: https://snyk.io/learn/container-security/

3. **Cloud Providers**
   - AWS ECS Best Practices: https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/
   - Google Cloud Run: https://cloud.google.com/run/docs/tips
   - Azure Container Instances: https://docs.microsoft.com/azure/container-instances/

### Herramientas √ötiles

| Herramienta | Prop√≥sito | URL |
|-------------|-----------|-----|
| **hadolint** | Lint Dockerfiles | https://github.com/hadolint/hadolint |
| **Trivy** | Vulnerability scanning | https://github.com/aquasecurity/trivy |
| **dive** | Analizar layers de imagen | https://github.com/wagoodman/dive |
| **docker-slim** | Reducir tama√±o de im√°genes | https://github.com/docker-slim/docker-slim |
| **Portainer** | UI para gesti√≥n de containers | https://www.portainer.io/ |
| **Watchtower** | Auto-update de containers | https://github.com/containrrr/watchtower |

### Comandos √ötiles

```bash
# ========================================
# Inspecci√≥n y Debug
# ========================================
# Ver layers de imagen
docker history myimage

# Analizar tama√±o por layer (con dive)
dive myimage

# Inspeccionar configuraci√≥n de imagen
docker inspect myimage

# Ejecutar shell en container corriendo
docker exec -it container_name /bin/sh

# Ver procesos en container
docker top container_name

# Ver stats en tiempo real
docker stats


# ========================================
# Limpieza
# ========================================
# Eliminar containers detenidos
docker container prune

# Eliminar im√°genes sin usar
docker image prune -a

# Eliminar vol√∫menes sin usar
docker volume prune

# Eliminar networks sin usar
docker network prune

# Limpieza completa (‚ö†Ô∏è CUIDADO)
docker system prune -a --volumes


# ========================================
# Build Optimization
# ========================================
# Build sin cache (debugging)
docker build --no-cache -t myimage .

# Build con BuildKit (m√°s r√°pido)
DOCKER_BUILDKIT=1 docker build -t myimage .

# Build con target espec√≠fico
docker build --target production -t myimage .


# ========================================
# Export/Import
# ========================================
# Exportar imagen a tar
docker save myimage:tag -o myimage.tar

# Importar imagen desde tar
docker load -i myimage.tar

# Exportar container a tar
docker export container_name -o container.tar

# Importar desde tar como imagen
docker import container.tar myimage:tag
```

---

## üéØ 13. CONCLUSI√ìN

### Principios Clave para Recordar

1. **üèóÔ∏è Multi-stage builds**: Separa build de runtime (60-90% reducci√≥n tama√±o)
2. **üîí Seguridad first**: Usuario no-root, secrets externos, escaneo continuo
3. **‚ö° Optimiza caching**: Orden correcto de COPY, RUN combinados
4. **üì¶ Minimalismo**: Solo lo necesario (slim/alpine/distroless)
5. **üè• Health checks**: Siempre implementar endpoints de salud
6. **üìä Observabilidad**: Logs estructurados, m√©tricas, tracing
7. **üéöÔ∏è Resource limits**: Proteger el sistema de runaway containers
8. **üîÑ Reproducibilidad**: Versiones espec√≠ficas, builds determin√≠sticos

### Checklist R√°pido

**Antes de cada build:**
- [ ] `.dockerignore` actualizado
- [ ] Secrets NO en c√≥digo
- [ ] Usuario no-root configurado
- [ ] Health check implementado

**Antes de desplegar:**
- [ ] Imagen escaneada (trivy/snyk)
- [ ] Tests pasando en container
- [ ] Resource limits configurados
- [ ] Logs y monitoring listos

### ROI Esperado

| M√©trica | Antes | Despu√©s | Mejora |
|---------|-------|---------|--------|
| Tama√±o imagen | ~1GB | ~150MB | **85%** ‚Üì |
| Build time | 8 min | 2 min | **75%** ‚Üì |
| Deploy time | 5 min | 1 min | **80%** ‚Üì |
| Vulnerabilities | 80+ | <5 | **94%** ‚Üì |
| Incident rate | Alta | Baja | **70%** ‚Üì |

---

**√öltima actualizaci√≥n:** 2025-10-02  
**Versi√≥n del documento:** 1.0  
**Mantenedor:** Equipo DevOps  
**Licencia:** MIT

---

## üìù Notas de Versi√≥n

### Filosof√≠a de Versionado de Este Documento

Este documento usa principios y patrones universales, NO versiones espec√≠ficas de software.

**Por qu√© agn√≥stico de versiones:**
- ‚úÖ Atemporal - V√°lido por a√±os
- ‚úÖ Flexible - Aplica a cualquier stack
- ‚úÖ Mantenible - No requiere actualizaciones constantes
- ‚úÖ Educativo - Ense√±a principios, no comandos memor√≠sticos

**C√≥mo usar este documento:**
1. Consulta documentaci√≥n oficial de tu stack para versiones actuales
2. Aplica los principios aqu√≠ documentados
3. Adapta ejemplos a tu contexto espec√≠fico
4. Mant√©n tus Dockerfiles bajo control de versiones

**Este documento evoluciona cuando:**
- Docker introduce nuevas features fundamentales
- Emergen nuevos patrones de seguridad cr√≠ticos
- Best practices de la industria cambian significativamente

**NO evoluciona cuando:**
- Sale una nueva versi√≥n de Python/Node/etc.
- Herramientas espec√≠ficas actualizan su CLI
- Cloud providers cambian pricing

---

**¬øDudas o sugerencias?** Abre un issue o PR en el repositorio del proyecto.

